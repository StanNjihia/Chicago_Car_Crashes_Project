{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b983173",
   "metadata": {},
   "source": [
    "# Chicago Car Crashes Project\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "### One of the purposes of having vehicle crash data is to; Better public safety, Improve urban planning, and Improve policy making\n",
    "\n",
    "### Some of the possible business questions that can be derived from the data include, \n",
    "\n",
    "#### 1. What factors contribute most to severe crashes?\n",
    "#### 2. Which locations, times, and conditions are accident prone?\n",
    "#### 3. Are certain groups more vulnerable to crashes?\n",
    "#### 4. Which vehcile types are most involved in severe or fatal crashes?\n",
    "#### 5. How do some behaviors impact crashes e.g seatbelt use, intoxication, or distractions affect injury severity?\n",
    "#### 6. How can the data being assessed be used to assist the police, hospitals, and city planners target interventions? \n",
    "\n",
    "## Problem Statement\n",
    "#### Pinpoint crash hotspots in Chicago and understand contributing factors to assists city planners and law enforcement to minimize accidents.\n",
    "\n",
    "## Metric for Success\n",
    "\n",
    "### Sucessfully answering the above business questions will be a a significant advantage.\n",
    "### Another metric will be making a hotspot analysis that accurately pinpoints high_risk zones for crashes. \n",
    "## Real World Use Case\n",
    "### could be in assisting governments and city planners on the regions that they need to install cameras, improve lighting, and redesign road structures in hotspots.\n",
    "\n",
    "\n",
    "### Considering that lives are involved and this is my first official model, an accuracy of 80% will be considered sufficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bab6b",
   "metadata": {},
   "source": [
    "# Modelling - Making A HotSpot Analysis\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90278a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import sklearn libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109aed7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets start with defining the target variable\n",
    "\n",
    "# the first step involves counting the crashes per zipcode \n",
    "\n",
    "zipcode_counts = cleaned_data.groupby(\"zipcode\").size().reset_index(name=\"crash_count\")\n",
    "\n",
    "# Then we can define the threshold for hotspots (Top 10% zipcodes)\n",
    "threshold = zipcode_counts[\"crash_count\"].quantile(0.9)\n",
    "\n",
    "# Creating a new hotspots column \n",
    "zipcode_counts[\"hotspot\"] = (zipcode_counts[\"crash_count\"] >= threshold).astype(int)\n",
    "\n",
    "# create a new dataset for modelling\n",
    "modelling_data = cleaned_data.merge(zipcode_counts[[\"zipcode\", \"hotspot\"]], on=\"zipcode\", how=\"left\")\n",
    "\n",
    "# preview the top ten crash_heavy zipcodes\n",
    "print(zipcode_counts.sort_values(\"crash_count\", ascending=False).head(10))\n",
    "\n",
    "# preview the merhed modelling dataset\n",
    "print(modelling_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb51cab",
   "metadata": {},
   "source": [
    "## Feature Engineering for the first model that predicts the areas that are prone to crashes per zipcode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make a copy of the cleaded data \n",
    "df = cleaned_data.copy()\n",
    "df = df[df[\"zipcode\"].notna()]\n",
    "\n",
    "current_year = pd.Timestamp.now().year\n",
    "\n",
    "features_by_zipcode = df.groupby(\"zipcode\").agg(\n",
    "    crash_count=(\"crash_record_id\", \"nunique\"),               # unique crashes per zipcode\n",
    "    avg_age=(\"age\", \"mean\"),\n",
    "    prop_male=(\"sex\", lambda x: (x == \"M\").mean()),\n",
    "    prop_missing_license=(\"drivers_license_class\", lambda x: x.isna().mean()),\n",
    "    prop_bac_positive=(\"bac_result\", lambda x: (x == \"Positive\").mean()),\n",
    "    driver_action_nunique=(\"driver_action\", \"nunique\"),\n",
    "    driver_vision_nunique=(\"driver_vision\", \"nunique\"),\n",
    "    prop_missing_physical_condition=(\"physical_condition\", lambda x: x.isna().mean()),\n",
    "    avg_vehicle_year=(\"vehicle_year\", \"mean\"),\n",
    "    avg_vehicle_age=(\"vehicle_year\", lambda x: (current_year - x).mean()),\n",
    "    prop_vehicle_defect=(\"vehicle_defect\", lambda x: x.notna().mean()),\n",
    "    vehicle_type_nunique=(\"vehicle_type\", \"nunique\"),\n",
    "    prop_airbag_deployed=(\"airbag_deployed\", lambda x: (x == \"Yes\").mean()),\n",
    "    avg_occupant_cnt=(\"occupant_cnt\", \"mean\"),\n",
    "    maneuver_nunique=(\"maneuver\", \"nunique\"),\n",
    "    travel_direction_nunique=(\"travel_direction\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "# Merge hotspot labels you created earlier\n",
    "modelling_data = features_by_zipcode.merge(\n",
    "    zipcode_counts[[\"zipcode\", \"hotspot\"]],\n",
    "    on=\"zipcode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(modelling_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b341f9",
   "metadata": {},
   "source": [
    "### Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd092b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "X = modelling_data.drop(columns=[\"hotspot\", \"zipcode\"])\n",
    "y = modelling_data[\"hotspot\"]\n",
    "\n",
    "# make the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afcd60",
   "metadata": {},
   "source": [
    "#### The results above shows that the feature engineering was successful since its purpose was to aggregate the 2 million person-level rows into zipcode-level features. The result was that it drastically reduced the dataset since Chicago has so many zipcodes. This will allow us to make predictions according to the existing zipcodes with the right data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to check if the data columns are indeed numeric\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46bf5a",
   "metadata": {},
   "source": [
    "#### Since from the above columns, it is clearly evident that the columns are all numeric, and can then continue with the rest of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the above confirms that the columns are numeric\n",
    "numeric_cols = X_train.columns\n",
    "\n",
    "# create a pipeline that scales the numeric features + logistic regression\n",
    "pipeline = Pipeline (steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# model fitting\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96606a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the models performance on training data\n",
    "print(f\"The model's score on training data is {pipeline.score(X_train, y_train)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2eea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the models prediction and accuracy\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "acc = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "print(f\"The model's accuracy on unseen data is {round(acc, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "sns.heatmap(conf, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e757c",
   "metadata": {},
   "source": [
    "#### The above confutions matrix shows on the top left the correctly predicted non-hotspots, the bottom right shows the cirrectly predicted hotspots. The bottom left and top right regions shows the existing misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c65ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b55fe",
   "metadata": {},
   "source": [
    "#### The above report shows that the class 0 shows the non-hotspot zipcodes, class 1 hotspot zipcodes. For the Non-hotspots, the precision is at a hundred%, the recall is at 95% which means that it catches 955 of all true non_hotspots, an f1-score of 97% whihc translates to a strong overall balance. From the presented figures making the conclusion that the model never mislabels a hotspot as a non_hotspot would be an accurate assessment.\n",
    "\n",
    "#### In the hotspots region, the precision score is at 70% which means that it is accurate 70% of the time with 30% of the instances being false alarms. The recall figures means that it catches about 97% of the real hotspots, the f1-score of 81% is also quite strong but quite lower to the non hotspots becaise of the drop in precision. \n",
    "\n",
    "#### The overall conclusion is that the model is excellent in finding hotspots, however, it has instances where it over-predicts and marks safe zipcodes as hotspots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004b7ba6",
   "metadata": {},
   "source": [
    "# Layman's Interpretation of the model\n",
    "\n",
    "#### The model is a binary classification model that predicts whether or not a zipcode is a crash hotspot. At this level I'm not sure whether I can predict future outcomes but I'm sure that I can classify whether an areas is a high-risk hotspot for crashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eee9df",
   "metadata": {},
   "source": [
    "## Business Interpretation \n",
    "\n",
    "#### The model is quite good at catching hotspots with a recall of 97%, but it still has instances where it gives false positives which is shows by the low precision score of 70% where it flags safe areas as hotspots. In the instance of the current problem statement, this isnt a bad trade-off since it may be better to over-predict a hotspot resulting in extra caution than missing a true hotspot, which will result in people getting hurt and in the owrst instances, deaths. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4288b5",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the random forest model\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# No need for another train-test split since it was already done above \n",
    "\n",
    "# model training\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# make predictions \n",
    "y_pred_train = rf_classifier.predict(X_train)\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance on unseen data\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f42683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification matric on the test set \n",
    "\n",
    "conf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(conf, annot=True, fmt=\"d\", xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "importances = pd.Series(rf_classifier.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=False).head(15).plot(kind=\"bar\", figsize=(10,5))\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e001d8",
   "metadata": {},
   "source": [
    "#### The above shows the importance score of each feature. Ideally, it assists in getting an understanding of how a specific feature reduces uncertainty and makes good splits across all trees. The higher the feature's score metric, the more that feature was used in making a string, discriminative decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20814e47",
   "metadata": {},
   "source": [
    "## Second model that predicts where and when crashes are likely to occur, I chose to go this route to see if I can be able to zone in on the regions which will allow planners to get a heat map of risk thats more predictive and not just descriptive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77549099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should start of by making spatial coordinates into 1KM grid sections\n",
    "grid_size = 0.01 \n",
    "merged_df[\"lat_bin\"] = (merged_df[\"latitude\"] // grid_size) * grid_size\n",
    "merged_df[\"lon_bin\"] = (merged_df[\"longitude\"] // grid_size) * grid_size\n",
    "\n",
    "# Define the features \n",
    "X = merged_df[[\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\", \"weather_condition\", \"lighting_condition\"]]\n",
    "\n",
    "# Defining the targets \n",
    "# We can also generate negative samples from random grid/time combos where crashes did not happen\n",
    "all_combos = pd.MultiIndex.from_product([\n",
    "    merged_df[\"lat_bin\"].unique(),\n",
    "    merged_df[\"lon_bin\"].unique(),\n",
    "    range(7),  # days of week\n",
    "    range(24)  # hours\n",
    "], names=[\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\"]) \n",
    "\n",
    "\n",
    "combo_df = pd.DataFrame(index=all_combos).reset_index()\n",
    "combo_df = combo_df.merge(merged_df.groupby([\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\"]).size().reset_index(name=\"crash_count\"),\n",
    "                          on=[\"lat_bin\",\"lon_bin\",\"crash_day_of_week\",\"crash_hour\"], how=\"left\")\n",
    "\n",
    "combo_df[\"crash_occurred\"] = (combo_df[\"crash_count\"].fillna(0) > 0).astype(int) \n",
    "\n",
    "# considering the features and targets\n",
    "X = combo_df[[\"lat_bin\",\"lon_bin\",\"crash_day_of_week\",\"crash_hour\"]]\n",
    "y = combo_df[\"crash_occurred\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd222c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical data\n",
    "X = X.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) \n",
    "\n",
    "#check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the baseline model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making evaluations on the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130163f0",
   "metadata": {},
   "source": [
    "Considering that the ROC score here is at 99%, that signifies good separation and reason to consider that data leakage did not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a656d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The model's score on training dataset is: {model.score(X_train, y_train)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d22c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy on unseen data\n",
    "\n",
    "print(f\"The model's score on unseen data is: {accuracy_score(y_test, y_pred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model seems to be fine, I will attempt to map the predictions where I will be taking the predicted probabilities and map them back\n",
    "\n",
    "X_test[\"pred_prob\"] = y_prob\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_test[\"lon_bin\"], X_test[\"lat_bin\"], c=X_test[\"pred_prob\"], cmap=\"Reds\", s=50)\n",
    "plt.colorbar(label=\"Predicted Crash Risk\")\n",
    "plt.title(\"Predicted Crash Hotspots in Chicago\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fc5b1",
   "metadata": {},
   "source": [
    "The purpose of the above heatmap is to give predicted crash risk which city planners can use to make proper planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872aff8",
   "metadata": {},
   "source": [
    "### Now, I already tried making a fourth model, tried going with XGBoost and LightGBM and my PC could no longer handle either of them at this point. Which forces me to settle with what I have at the moment. Unfortunately, the same applies for hyperparemter tuning. Which will force me to go straight to storytelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90bfc26",
   "metadata": {},
   "source": [
    "# Storytelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd4430",
   "metadata": {},
   "source": [
    "## 1. Problem objective\n",
    "#### Chicago experiences plenty of traffic crashes annually which ends up negatively impacting lives, property, and urban efficiency. As a way to best plan and reduce the crashes what can be done is to pinpoint where crashes cluster colliqually termed as black spots, and the factors that drive teh same, so that resources and the causes can be assessed to reduce accidents. Ideally, that is in line with the problem statement \"To pinpoint high-risk locations and uncover the human, vehicle, and environmental conditions that are majorly associated to accidents.\"\n",
    "\n",
    "## 2. Data Exploration (Key Insights)\n",
    "#### From the analysis, most accidents happen during rush hour periods which is around 3PM to 7PM with the weekends having variations but a common factor being that they occur at night, but Saturdays being a constant figure with the highest number of crashes. While considering the crash severity, most crashes majorly involved property damage with a many having injuries and more interestingly for a 2million crash data low deaths which is actually commendable. Most crashes involve passenge type vehicles with motorcycle crashes and trucks having a low count but have significanlty higher severity rates. \n",
    "\n",
    "## 3. Patterns and Hotspots\n",
    "#### Altough I'm not consident with my spatial interpretation, I will still do my best since it involves visual cues,it shouldnt be that difficult. Now, the crash density is not quite uniform with majority of it being concentrated in the dowtown regions with dense traffic and pedestrians, major intersection and arterial roads, and entertainment districts which majorly happends during weekends. \n",
    "\n",
    "## 4. Contributing Factors\n",
    "#### Human factors involve situations where the driver was distracted while driving and alcohol related crashes spike at night, at the same time the you younger drivers are quite overrepresented in injury-causing crashes. Vehicle factors such as SUVs and Sedans dominate the number of high accidentc, and crashes involving motorcycles have the highest injury severity rate, Ironically when it comes to environmental factors, most crashes occur when the weather is mostly clear which asserts the notion that bad driving behaviour matters more than rain or snow. Also, poorly lit areas or intersections have significantly higher crash risks which was a very expected insight.\n",
    "\n",
    "## 5. Predictive Modeling \n",
    "#### While descriptive analysis identifies past crash hotspots, we also explored whether machine learning can flag likely future hotspots based on location and crash attributes. Defined hotspots as the top 5% most crash-dense grid cells.\n",
    "#### Features included time, location bins, vehicle type, driver attributes, weather, and lighting.\n",
    "#### Tested models: Logistic Regression, XGBoost, LightGBM (baseline implementation).\n",
    "#### Result:\n",
    "#### Models reached ~85â€“90% accuracy, but struggled with precision on rare hotspots (too few positives compared to negatives).\n",
    "#### Interpretation: Predicting exact hotspot cells is difficult without richer urban features (traffic flow, road geometry, enforcement data).\n",
    "\n",
    "## 6. Recommendations & Next Steps\n",
    "#### The data highlights clear interventions for the city and for further research.\n",
    "\n",
    "#### City-Level Recommendations:\n",
    "\n",
    "#### Target Enforcement & Awareness\n",
    "\n",
    "#### Focus traffic enforcement during evening rush hours and weekend nights.\n",
    "\n",
    "#### Prioritize young drivers with awareness campaigns.\n",
    "\n",
    "#### Infrastructure Improvements\n",
    "\n",
    "#### Invest in better lighting at crash-prone intersections.\n",
    "\n",
    "#### Redesign top hotspot intersections (traffic calming, pedestrian protection).\n",
    "\n",
    "#### Data-Driven Policy\n",
    "\n",
    "#### Require ride-share & delivery services to share driving data (these fleets may be overrepresented).\n",
    "\n",
    "#### Consider stricter penalties for night-time alcohol-related driving.\n",
    "\n",
    "#### Analytical Next Steps:\n",
    "\n",
    "#### Integrate road network data (speed limits, lane counts).\n",
    "\n",
    "#### Include traffic volume data (exposure-adjusted risk, not just raw crash counts).\n",
    "\n",
    "#### Expand prediction work with deep learning spatial models if data allows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_Moringa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
