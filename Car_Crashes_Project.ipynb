{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfded7a6",
   "metadata": {},
   "source": [
    "# Chicago Car Crashes Project\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "### One of the purposes of having vehicle crash data is to; Better public safety, Improve urban planning, and Improve policy making\n",
    "\n",
    "### Some of the possible business questions that can be derived from the data include, \n",
    "\n",
    "#### 1. What factors contribute most to severe crashes?\n",
    "#### 2. Which locations, times, and conditions are accident prone?\n",
    "#### 3. Are certain groups more vulnerable to crashes?\n",
    "#### 4. Which vehcile types are most involved in severe or fatal crashes?\n",
    "#### 5. How do some behaviors impact crashes e.g seatbelt use, intoxication, or distractions affect injury severity?\n",
    "#### 6. How can the data being assessed be used to assist the police, hospitals, and city planners target interventions? \n",
    "\n",
    "## Problem Statement\n",
    "#### Pinpoint crash hotspots in Chicago and understand contributing factors to assists city planners and law enforcement to minimize accidents.\n",
    "\n",
    "## Metric for Success\n",
    "\n",
    "### Sucessfully answering the above business questions will be a a significant advantage.\n",
    "### Another metric will be making a hotspot analysis that accurately pinpoints high_risk zones for crashes. \n",
    "## Real World Use Case\n",
    "### could be in assisting governments and city planners on the regions that they need to install cameras, improve lighting, and redesign road structures in hotspots.\n",
    "\n",
    "\n",
    "### Considering that lives are involved and this is my first official model, an accuracy of 80% will be considered sufficient.\n",
    "\n",
    "## Data Understanding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07301e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#import sklearn libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, SMOTEN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report,roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4753b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "people_data = pd.read_csv(\"Traffic_Crashes_People.csv\", low_memory=False)\n",
    "vehicles_data = pd.read_csv(\"Traffic_Crashes_Vehicles.csv\", low_memory=False)\n",
    "crashes_data = pd.read_csv(\"Traffic_Crashes_Crashes.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the people dataset\n",
    "people_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f11974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the vehicles dataset\n",
    "vehicles_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the vehicles dataset\n",
    "crashes_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63114052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape on both datasets\n",
    "print(f\"The people dataset has a shape of {people_data.shape}, the vehicles dataset has a shape of {vehicles_data.shape}, the crashes dataset has a shape of {crashes_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4cefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the column names in both datasets\n",
    "print(people_data.columns)\n",
    "\n",
    "print(vehicles_data.columns)\n",
    "\n",
    "print(crashes_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7af30",
   "metadata": {},
   "source": [
    "#### From the above, the vehicles dataset has a lot more columns than the peoples dataset. \n",
    "#### However, I will still try to merge the two datasets so that I can start working from a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The approach for joining the two datasets will be to join on the crash record ID and vehicle ID column since they are similar in both sets.\n",
    "# This approach will help in minimizing any cases of duplicates\n",
    "\n",
    "# The first part of this approach is to standardise the columns to avoid any mismatches that may occur\n",
    "people_data.columns = people_data.columns.str.lower()\n",
    "vehicles_data.columns = vehicles_data.columns.str.lower() \n",
    "crashes_data.columns = crashes_data.columns.str.lower()\n",
    "# Next, merging on crash record ID and vehcile ID columns\n",
    "\n",
    "\n",
    "# Disclaimer, as a result of the meagre computing power on my pc, I decided to go with inner merge. \n",
    "people_vehicles = pd.merge(\n",
    "    people_data,\n",
    "    vehicles_data,\n",
    "    how=\"inner\",\n",
    "    on=[\"crash_record_id\", \"vehicle_id\"]\n",
    ")\n",
    "\n",
    "# Merging the third dataset\n",
    "merged_df = pd.merge(    \n",
    "    people_vehicles,\n",
    "    crashes_data,\n",
    "    how=\"inner\",\n",
    "    on=\"crash_record_id\"\n",
    ")\n",
    "\n",
    "\n",
    "print (merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the first 5 columns on the merged data\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the percentage of missing data\n",
    "missing_data = merged_df.isnull().mean()*100\n",
    "\n",
    "#  Sorting the missing data in descending order\n",
    "missing_data = missing_data.sort_values(ascending=False)\n",
    "\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa778e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now considering the number of rows and columns, I will be dropping the columns with more than 70% missing data. \n",
    "# The 70% figure is based on the fact that there is too much missing data for that column to be useful.\n",
    "\n",
    "# Threshold (70%)\n",
    "threshold = 0.7\n",
    "\n",
    "# Drop columns where more than 70% values are missing\n",
    "merged_df = merged_df.loc[:, merged_df.isnull().mean() < threshold]\n",
    "\n",
    "# Checking the percentage of missing data\n",
    "missing_data = merged_df.isnull().mean()*100\n",
    "\n",
    "#  Sorting the missing data in descending order\n",
    "missing_data = missing_data.sort_values(ascending=False)\n",
    "\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93482e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data's new shape\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6fd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names to check for typos or different names\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132055c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for the remaining columns, I will try to use relationships and columns to fill in the missing data\n",
    "\n",
    "corrs = merged_df.corr(numeric_only=True)[\"num_passengers\"].sort_values(ascending=False)\n",
    "\n",
    "print(corrs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is a high correlation with occupant_cnt, I will start by filling in that column, which will the be used to fill in the num_passengers column\n",
    "\n",
    "# Filling in occupant_cnt using mode\n",
    "mode_value = merged_df[\"occupant_cnt\"].mode()[0]\n",
    "\n",
    "merged_df[\"occupant_cnt\"] = merged_df[\"occupant_cnt\"].fillna(mode_value)\n",
    "\n",
    "# check if it took\n",
    "merged_df[\"occupant_cnt\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the occupant cnt column to fill in the num passengers\n",
    "\n",
    "merged_df[\"num_passengers\"] = merged_df.apply(\n",
    "    lambda row: row[\"occupant_cnt\"] - 1\n",
    "    if pd.isna(row[\"num_passengers\"]) else row[\"num_passengers\"], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# check if it took \n",
    "merged_df[\"num_passengers\"].isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ed52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling in the data from columns\n",
    "columns_to_fill_with_mode = [\n",
    "    \"drivers_license_class\", \"drivers_license_state\", \"zipcode\", \"age\",\"city\", \"state\", \"driver_vision\", \"driver_action\", \"bac_result\",\n",
    "    \"physical_condition\", \"vehicle_year\", \"lic_plate_state\", \"first_contact_point\",\"model\", \"make\", \"occupant_cnt\", \"maneuver\", \"travel_direction\",\n",
    "    \"vehicle_use\", \"vehicle_type\", \"vehicle_defect\", \"vehicle_id\",\"airbag_deployed\", \"sex\", \"ejection\", \"safety_equipment\", \"injury_classification\",\n",
    "    \"unit_type\",\"report_type\",\"location\",\"longitude\",\"latitude\",\"most_severe_injury\",\"beat_of_occurrence\",\"street_direction\",\"street_name\" \n",
    "]\n",
    "\n",
    "# Looping through and filling each with the mode \n",
    "\n",
    "for col in columns_to_fill_with_mode:\n",
    "    mode_value = merged_df[col].mode()[0]   # get most common value\n",
    "    merged_df[col] = merged_df[col].fillna(mode_value)\n",
    "\n",
    "# Check one of them\n",
    "print(\"Missing drivers_license_class:\", merged_df[\"drivers_license_class\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for columns in missing data\n",
    "missing_data2 = merged_df.isnull().mean()*100\n",
    "\n",
    "#  Sorting the missing data in descending order\n",
    "missing_data2 = missing_data2.sort_values(ascending=False)\n",
    "\n",
    "print(missing_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm the imputation\n",
    "merged_df.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates\n",
    "merged_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35c83f",
   "metadata": {},
   "source": [
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de12cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the cleaned dataset\n",
    "\n",
    "cleaned_data = merged_df.copy(deep=True)\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for outliers\n",
    "sns.boxplot(cleaned_data,color=\"r\")\n",
    "plt.tight_layout()\n",
    "plt.grid(alpha=.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40774810",
   "metadata": {},
   "source": [
    "From the plot above, the vehicle_Id and crash_unit are unique identifiers assigned to each record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d48ad",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d711046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering the size of the dataset, the next step will be to get a sample of 10% of the data of randomly selected rows to use for analysis\n",
    "sample_data = cleaned_data.sample(frac=0.1, random_state=42)\n",
    "sample_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605d766",
   "metadata": {},
   "source": [
    "##### The above creates a sample size of 200K which allows me to conduct my analysis in a way that my P.C can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de17258",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering my problem statement and maintaining the same, EDA will focus along those lines\n",
    "\n",
    "# Making sure that the 'latitude' and 'longitude' columns do not have any unfilled sections\n",
    "df_spatial = sample_data.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# To make sure that both the latitude and longitude only cover the Chicago region\n",
    "df_spatial = df_spatial[\n",
    "    (df_spatial['latitude'].between(41.6, 42.1)) &\n",
    "    (df_spatial['longitude'].between(-87.95, -87.5))\n",
    "]\n",
    "\n",
    "print(f\"Remaining rows after filtering: {len(df_spatial)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static density visualization which shows a static heatmap where the crashes are concentrated\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.hexbin(df_spatial['longitude'], df_spatial['latitude'], gridsize=100, cmap='Reds', bins='log')\n",
    "plt.colorbar(label='log(crash count)')\n",
    "plt.title(\"Crash Density Across Chicago\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hotspot clustering using DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "coords = df_spatial[['latitude','longitude']].to_numpy()\n",
    "\n",
    "# scaling for DBSCAN\n",
    "coords_scaled = StandardScaler().fit_transform(coords)\n",
    "\n",
    "db = DBSCAN(eps=0.05, min_samples=30).fit(coords_scaled)  \n",
    "df_spatial['cluster'] = db.labels_\n",
    "\n",
    "print(df_spatial['cluster'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8937b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the hotspots\n",
    "hotspots = df_spatial.groupby('cluster').agg(\n",
    "    crashes=('crash_record_id', 'count'),\n",
    "    total_injuries=('injuries_total','sum'),\n",
    "    fatalities=('injuries_fatal','sum')\n",
    ").sort_values('crashes', ascending=False)\n",
    "\n",
    "# Checking the order of the hostspots\n",
    "hotspots.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a70c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the cluster on a map with the crashes being color-coded according to the clusters\n",
    "import folium\n",
    "m_clusters = folium.Map(location=[41.85, -87.65], zoom_start=11)\n",
    "\n",
    "for _, row in df_spatial.iterrows():\n",
    "    if row['cluster'] != -1:\n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=2,\n",
    "            color=f\"#{(hash(row['cluster']) & 0xFFFFFF):06x}\",  # cluster color\n",
    "            fill=True,\n",
    "            fill_opacity=0.6\n",
    "        ).add_to(m_clusters)\n",
    "\n",
    "m_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc0a5b",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b66e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lines of code are meant to structure the relevant variables according to the hotspot issues instead of brutforcing all the columns \n",
    "# and data which are quite substantial\n",
    "\n",
    "# Detailing the appropriate code for making the necessary plots for the rest of the workflow.\n",
    "\n",
    "def plot_categorical(series, top_n=10):\n",
    "    counts = series.value_counts().head(top_n)\n",
    "    sns.barplot(x=counts.values, y=counts.index)\n",
    "    plt.title(series.name)\n",
    "    plt.show();\n",
    "\n",
    "def plot_numeric(series, bins=20):\n",
    "    sns.histplot(series.dropna(), bins=bins, kde=False)\n",
    "    plt.title(series.name)\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the time when ost crashes occur\n",
    "plot_numeric(sample_data['crash_hour'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ec24c",
   "metadata": {},
   "source": [
    "From the above, the distribution is according to the period or hour when most crashes tend to occur with most of the crashes happening at 3pm followed by 7 am, and around midnight. This information can help us better explain some of the reasons being the 3pm jam that occurs as most people are leaving work, conducting their errands, and picking children up from school. The 7AM crashes can be explained as the early morning bustle as people try to go to work and school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the days of the week if stored as numbers\n",
    "day_map = {\n",
    "    1: \"Monday\", 2: \"Tuesday\", 3: \"Wednesday\",\n",
    "    4: \"Thursday\", 5: \"Friday\", 6: \"Saturday\", 7: \"Sunday\"\n",
    "}\n",
    "sample_data['crash_day_of_week'] = sample_data['crash_day_of_week'].map(day_map)\n",
    "\n",
    "# Ensure categorical order\n",
    "days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "sample_data['crash_day_of_week'] = pd.Categorical(\n",
    "    sample_data['crash_day_of_week'],\n",
    "    categories=days_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Plot\n",
    "sns.countplot(data=sample_data, y='crash_day_of_week', order=days_order)\n",
    "plt.title(\"Crashes by Day of Week\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de676508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month map: 1-12 → Month names\n",
    "month_map = {\n",
    "    1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\",\n",
    "    5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\",\n",
    "    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "sample_data['crash_month'] = sample_data['crash_month'].map(month_map)\n",
    "\n",
    "# Chronological order (Jan → Dec)\n",
    "month_order = list(month_map.values())\n",
    "\n",
    "# Plot with order\n",
    "sns.countplot(\n",
    "    data=merged_df,\n",
    "    x='crash_month',\n",
    "    order=month_order,\n",
    "    palette=\"crest\"\n",
    ")\n",
    "plt.title(\"Crashes by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Crashes\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consideirng the deverity of the crashes\n",
    "# The most severe injuries\n",
    "plot_categorical(sample_data['most_severe_injury'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(sample_data['damage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76908b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assessment of the weather conditions and when most accidents occured\n",
    "\n",
    "plot_categorical(merged_df['weather_condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assessment of the lighting conditions when most accidents occured\n",
    "\n",
    "plot_categorical(merged_df['lighting_condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccf11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The existence of traffic controls whenever accidents occurred\n",
    "\n",
    "plot_categorical(merged_df['traffic_control_device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff65827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The road surface conditions whenever accidents occured and when most accidents occurred\n",
    "\n",
    "plot_categorical(merged_df['roadway_surface_cond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac79f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_categorical(merged_df['prim_contributory_cause'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b18071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assessment of the gender that led to the most accidents\n",
    "\n",
    "plot_categorical(merged_df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f554ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting an assessment of the ages of people involved in the accidents\n",
    "\n",
    "plot_numeric(merged_df[merged_df['age'] > 0]['age'], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c41f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances when the people involved in accidents had worn safety equipment\n",
    "\n",
    "plot_categorical(merged_df['safety_equipment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether or not BAC tests were administered\n",
    "\n",
    "plot_categorical(merged_df['bac_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a582a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The type of vehicle involved in an accident\n",
    "\n",
    "plot_categorical(merged_df['vehicle_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether or not the vehicle had any defects\n",
    "\n",
    "plot_categorical(merged_df['vehicle_defect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6f028",
   "metadata": {},
   "source": [
    "# Bivariate Analysis\n",
    "\n",
    "## An assessment of the relationship between features.\n",
    "### For this section I will majorly focus on elements that majorly align with my problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd563e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age vs injury classification \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=sample_data, x='injury_classification', y='age')\n",
    "plt.title(\"Driver Age vs Injury Classification\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8677347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assessment of the vehicle type vs injury classification\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=sample_data, x='vehicle_type', hue='injury_classification',\n",
    "              order=sample_data['vehicle_type'].value_counts().iloc[:10].index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Vehicle Type vs Injury Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6991764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assessment of the relationship between alcoholism and injury\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=sample_data, x='bac_result', hue='injury_classification')\n",
    "plt.title(\"ALCOHOL (BAC) Result vs Injury Classification\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ae7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your crash_hour is already in the right format - just use it directly!\n",
    "simple_grouped = merged_df.groupby('crash_hour').size()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "simple_grouped.plot(kind='bar')\n",
    "plt.title('Traffic Crashes by Hour of Day')\n",
    "plt.xlabel('Hour (24-hour format)')\n",
    "plt.ylabel('Number of Crashes')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af79fc",
   "metadata": {},
   "source": [
    "# Multivariate Analysis\n",
    "### Interactions between multiple variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering Vehicle Year, Age, and Injury\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.scatterplot(data=sample_data, x='vehicle_year', y='age', hue='injury_classification', alpha=0.6)\n",
    "plt.title(\"Vehicle Year vs Age colored by Injury Classification\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a19af",
   "metadata": {},
   "source": [
    "# A correlation Heatmap of the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f97d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(sample_data[['age','num_passengers','vehicle_year','occupant_cnt']].corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec87c1",
   "metadata": {},
   "source": [
    "# Modelling - Making A HotSpot Analysis\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with defining the target variable\n",
    "\n",
    "# the first step involves counting the crashes per zipcode \n",
    "\n",
    "zipcode_counts = cleaned_data.groupby(\"zipcode\").size().reset_index(name=\"crash_count\")\n",
    "\n",
    "# Then we can define the threshold for hotspots (Top 10% zipcodes)\n",
    "threshold = zipcode_counts[\"crash_count\"].quantile(0.9)\n",
    "\n",
    "# Creating a new hotspots column \n",
    "zipcode_counts[\"hotspot\"] = (zipcode_counts[\"crash_count\"] >= threshold).astype(int)\n",
    "\n",
    "# create a new dataset for modelling\n",
    "modelling_data = cleaned_data.merge(zipcode_counts[[\"zipcode\", \"hotspot\"]], on=\"zipcode\", how=\"left\")\n",
    "\n",
    "# preview the top ten crash_heavy zipcodes\n",
    "print(zipcode_counts.sort_values(\"crash_count\", ascending=False).head(10))\n",
    "\n",
    "# preview the merhed modelling dataset\n",
    "print(modelling_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591518ba",
   "metadata": {},
   "source": [
    "## Feature Engineering for the first model that predicts the areas that are prone to crashes per zipcode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e4624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make a copy of the cleaded data \n",
    "df = cleaned_data.copy()\n",
    "df = df[df[\"zipcode\"].notna()]\n",
    "\n",
    "current_year = pd.Timestamp.now().year\n",
    "\n",
    "features_by_zipcode = df.groupby(\"zipcode\").agg(\n",
    "    crash_count=(\"crash_record_id\", \"nunique\"),               # unique crashes per zipcode\n",
    "    avg_age=(\"age\", \"mean\"),\n",
    "    prop_male=(\"sex\", lambda x: (x == \"M\").mean()),\n",
    "    prop_missing_license=(\"drivers_license_class\", lambda x: x.isna().mean()),\n",
    "    prop_bac_positive=(\"bac_result\", lambda x: (x == \"Positive\").mean()),\n",
    "    driver_action_nunique=(\"driver_action\", \"nunique\"),\n",
    "    driver_vision_nunique=(\"driver_vision\", \"nunique\"),\n",
    "    prop_missing_physical_condition=(\"physical_condition\", lambda x: x.isna().mean()),\n",
    "    avg_vehicle_year=(\"vehicle_year\", \"mean\"),\n",
    "    avg_vehicle_age=(\"vehicle_year\", lambda x: (current_year - x).mean()),\n",
    "    prop_vehicle_defect=(\"vehicle_defect\", lambda x: x.notna().mean()),\n",
    "    vehicle_type_nunique=(\"vehicle_type\", \"nunique\"),\n",
    "    prop_airbag_deployed=(\"airbag_deployed\", lambda x: (x == \"Yes\").mean()),\n",
    "    avg_occupant_cnt=(\"occupant_cnt\", \"mean\"),\n",
    "    maneuver_nunique=(\"maneuver\", \"nunique\"),\n",
    "    travel_direction_nunique=(\"travel_direction\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "# Merge hotspot labels you created earlier\n",
    "modelling_data = features_by_zipcode.merge(\n",
    "    zipcode_counts[[\"zipcode\", \"hotspot\"]],\n",
    "    on=\"zipcode\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(modelling_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b590259",
   "metadata": {},
   "source": [
    "### Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c216bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "X = modelling_data.drop(columns=[\"hotspot\", \"zipcode\"])\n",
    "y = modelling_data[\"hotspot\"]\n",
    "\n",
    "# make the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574f42b",
   "metadata": {},
   "source": [
    "#### The results above shows that the feature engineering was successful since its purpose was to aggregate the 2 million person-level rows into zipcode-level features. The result was that it drastically reduced the dataset since Chicago has so many zipcodes. This will allow us to make predictions according to the existing zipcodes with the right data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to check if the data columns are indeed numeric\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabda2a2",
   "metadata": {},
   "source": [
    "#### Since from the above columns, it is clearly evident that the columns are all numeric, and can then continue with the rest of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f586a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the above confirms that the columns are numeric\n",
    "numeric_cols = X_train.columns\n",
    "\n",
    "# create a pipeline that scales the numeric features + logistic regression\n",
    "pipeline = Pipeline (steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# model fitting\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the models performance on training data\n",
    "print(f\"The model's score on training data is {pipeline.score(X_train, y_train)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing the models prediction and accuracy\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "acc = accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "print(f\"The model's accuracy on unseen data is {round(acc, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Confusion Matrix\")\n",
    "sns.heatmap(conf, annot=True, fmt=\"d\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3c63f",
   "metadata": {},
   "source": [
    "#### The above confutions matrix shows on the top left the correctly predicted non-hotspots, the bottom right shows the cirrectly predicted hotspots. The bottom left and top right regions shows the existing misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02effe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7710fb",
   "metadata": {},
   "source": [
    "#### The above report shows that the class 0 shows the non-hotspot zipcodes, class 1 hotspot zipcodes. For the Non-hotspots, the precision is at a hundred%, the recall is at 95% which means that it catches 955 of all true non_hotspots, an f1-score of 97% whihc translates to a strong overall balance. From the presented figures making the conclusion that the model never mislabels a hotspot as a non_hotspot would be an accurate assessment.\n",
    "\n",
    "#### In the hotspots region, the precision score is at 70% which means that it is accurate 70% of the time with 30% of the instances being false alarms. The recall figures means that it catches about 97% of the real hotspots, the f1-score of 81% is also quite strong but quite lower to the non hotspots becaise of the drop in precision. \n",
    "\n",
    "#### The overall conclusion is that the model is excellent in finding hotspots, however, it has instances where it over-predicts and marks safe zipcodes as hotspots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1275f",
   "metadata": {},
   "source": [
    "# Layman's Interpretation of the model\n",
    "\n",
    "#### The model is a binary classification model that predicts whether or not a zipcode is a crash hotspot. At this level I'm not sure whether I can predict future outcomes but I'm sure that I can classify whether an areas is a high-risk hotspot for crashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa71fb",
   "metadata": {},
   "source": [
    "## Business Interpretation \n",
    "\n",
    "#### The model is quite good at catching hotspots with a recall of 97%, but it still has instances where it gives false positives which is shows by the low precision score of 70% where it flags safe areas as hotspots. In the instance of the current problem statement, this isnt a bad trade-off since it may be better to over-predict a hotspot resulting in extra caution than missing a true hotspot, which will result in people getting hurt and in the owrst instances, deaths. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d833a",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the random forest model\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# No need for another train-test split since it was already done above \n",
    "\n",
    "# model training\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# make predictions \n",
    "y_pred_train = rf_classifier.predict(X_train)\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c37c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance on unseen data\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22833f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification matric on the test set \n",
    "\n",
    "conf = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(conf, annot=True, fmt=\"d\", xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Random Forest Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "\n",
    "importances = pd.Series(rf_classifier.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=False).head(15).plot(kind=\"bar\", figsize=(10,5))\n",
    "plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca810e0",
   "metadata": {},
   "source": [
    "#### The above shows the importance score of each feature. Ideally, it assists in getting an understanding of how a specific feature reduces uncertainty and makes good splits across all trees. The higher the feature's score metric, the more that feature was used in making a string, discriminative decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc67493",
   "metadata": {},
   "source": [
    "## Second model that predicts where and when crashes are likely to occur, I chose to go this route to see if I can be able to zone in on the regions which will allow planners to get a heat map of risk thats more predictive and not just descriptive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28182649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should start of by making spatial coordinates into 1KM grid sections\n",
    "grid_size = 0.01 \n",
    "merged_df[\"lat_bin\"] = (merged_df[\"latitude\"] // grid_size) * grid_size\n",
    "merged_df[\"lon_bin\"] = (merged_df[\"longitude\"] // grid_size) * grid_size\n",
    "\n",
    "# Define the features \n",
    "X = merged_df[[\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\", \"weather_condition\", \"lighting_condition\"]]\n",
    "\n",
    "# Defining the targets \n",
    "# We can also generate negative samples from random grid/time combos where crashes did not happen\n",
    "all_combos = pd.MultiIndex.from_product([\n",
    "    merged_df[\"lat_bin\"].unique(),\n",
    "    merged_df[\"lon_bin\"].unique(),\n",
    "    range(7),  # days of week\n",
    "    range(24)  # hours\n",
    "], names=[\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\"]) \n",
    "\n",
    "\n",
    "combo_df = pd.DataFrame(index=all_combos).reset_index()\n",
    "combo_df = combo_df.merge(merged_df.groupby([\"lat_bin\", \"lon_bin\", \"crash_day_of_week\", \"crash_hour\"]).size().reset_index(name=\"crash_count\"),\n",
    "                          on=[\"lat_bin\",\"lon_bin\",\"crash_day_of_week\",\"crash_hour\"], how=\"left\")\n",
    "\n",
    "combo_df[\"crash_occurred\"] = (combo_df[\"crash_count\"].fillna(0) > 0).astype(int) \n",
    "\n",
    "# considering the features and targets\n",
    "X = combo_df[[\"lat_bin\",\"lon_bin\",\"crash_day_of_week\",\"crash_hour\"]]\n",
    "y = combo_df[\"crash_occurred\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b257bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the categorical data\n",
    "X = X.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) \n",
    "\n",
    "#check shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89261497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the baseline model\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666313b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making evaluations on the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998507e",
   "metadata": {},
   "source": [
    "Considering that the ROC score here is at 99%, that signifies good separation and reason to consider that data leakage did not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d4f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The model's score on training dataset is: {model.score(X_train, y_train)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc05863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy on unseen data\n",
    "\n",
    "print(f\"The model's score on unseen data is: {accuracy_score(y_test, y_pred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f95999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model seems to be fine, I will attempt to map the predictions where I will be taking the predicted probabilities and map them back\n",
    "\n",
    "X_test[\"pred_prob\"] = y_prob\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_test[\"lon_bin\"], X_test[\"lat_bin\"], c=X_test[\"pred_prob\"], cmap=\"Reds\", s=50)\n",
    "plt.colorbar(label=\"Predicted Crash Risk\")\n",
    "plt.title(\"Predicted Crash Hotspots in Chicago\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a0614",
   "metadata": {},
   "source": [
    "The purpose of the above heatmap is to give predicted crash risk which city planners can use to make proper planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b219e1",
   "metadata": {},
   "source": [
    "### Now, I already tried making a fourth model, tried going with XGBoost and LightGBM and my PC could no longer handle either of them at this point. Which forces me to settle with what I have at the moment. Unfortunately, the same applies for hyperparemter tuning. Which will force me to go straight to storytelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c796a93",
   "metadata": {},
   "source": [
    "# Storytelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c339662",
   "metadata": {},
   "source": [
    "## 1. Problem objective\n",
    "#### Chicago experiences plenty of traffic crashes annually which ends up negatively impacting lives, property, and urban efficiency. As a way to best plan and reduce the crashes what can be done is to pinpoint where crashes cluster colliqually termed as black spots, and the factors that drive teh same, so that resources and the causes can be assessed to reduce accidents. Ideally, that is in line with the problem statement \"To pinpoint high-risk locations and uncover the human, vehicle, and environmental conditions that are majorly associated to accidents.\"\n",
    "\n",
    "## 2. Data Exploration (Key Insights)\n",
    "#### From the analysis, most accidents happen during rush hour periods which is around 3PM to 7PM with the weekends having variations but a common factor being that they occur at night, but Saturdays being a constant figure with the highest number of crashes. While considering the crash severity, most crashes majorly involved property damage with a many having injuries and more interestingly for a 2million crash data low deaths which is actually commendable. Most crashes involve passenge type vehicles with motorcycle crashes and trucks having a low count but have significanlty higher severity rates. \n",
    "\n",
    "## 3. Patterns and Hotspots\n",
    "#### Altough I'm not consident with my spatial interpretation, I will still do my best since it involves visual cues,it shouldnt be that difficult. Now, the crash density is not quite uniform with majority of it being concentrated in the dowtown regions with dense traffic and pedestrians, major intersection and arterial roads, and entertainment districts which majorly happends during weekends. \n",
    "\n",
    "## 4. Contributing Factors\n",
    "#### Human factors involve situations where the driver was distracted while driving and alcohol related crashes spike at night, at the same time the you younger drivers are quite overrepresented in injury-causing crashes. Vehicle factors such as SUVs and Sedans dominate the number of high accidentc, and crashes involving motorcycles have the highest injury severity rate, Ironically when it comes to environmental factors, most crashes occur when the weather is mostly clear which asserts the notion that bad driving behaviour matters more than rain or snow. Also, poorly lit areas or intersections have significantly higher crash risks which was a very expected insight.\n",
    "\n",
    "## 5. Predictive Modeling \n",
    "#### While descriptive analysis identifies past crash hotspots, we also explored whether machine learning can flag likely future hotspots based on location and crash attributes. Defined hotspots as the top 5% most crash-dense grid cells.\n",
    "#### Features included time, location bins, vehicle type, driver attributes, weather, and lighting.\n",
    "#### Tested models: Logistic Regression, XGBoost, LightGBM (baseline implementation).\n",
    "#### Result:\n",
    "#### Models reached ~85–90% accuracy, but struggled with precision on rare hotspots (too few positives compared to negatives).\n",
    "#### Interpretation: Predicting exact hotspot cells is difficult without richer urban features (traffic flow, road geometry, enforcement data).\n",
    "\n",
    "## 6. Recommendations & Next Steps\n",
    "#### The data highlights clear interventions for the city and for further research.\n",
    "\n",
    "#### City-Level Recommendations:\n",
    "\n",
    "#### Target Enforcement & Awareness\n",
    "\n",
    "#### Focus traffic enforcement during evening rush hours and weekend nights.\n",
    "\n",
    "#### Prioritize young drivers with awareness campaigns.\n",
    "\n",
    "#### Infrastructure Improvements\n",
    "\n",
    "#### Invest in better lighting at crash-prone intersections.\n",
    "\n",
    "#### Redesign top hotspot intersections (traffic calming, pedestrian protection).\n",
    "\n",
    "#### Data-Driven Policy\n",
    "\n",
    "#### Require ride-share & delivery services to share driving data (these fleets may be overrepresented).\n",
    "\n",
    "#### Consider stricter penalties for night-time alcohol-related driving.\n",
    "\n",
    "#### Analytical Next Steps:\n",
    "\n",
    "#### Integrate road network data (speed limits, lane counts).\n",
    "\n",
    "#### Include traffic volume data (exposure-adjusted risk, not just raw crash counts).\n",
    "\n",
    "#### Expand prediction work with deep learning spatial models if data allows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0930d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f96877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_Moringa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
